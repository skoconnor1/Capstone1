{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from datetime import datetime\n",
    "from scipy.stats import stats\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df, start_date, split_date):\n",
    "    training_data = df[df.Date < split_date]\n",
    "\n",
    "    #remove April 2009 data because there are a lot of nulls\n",
    "    training_data = training_data[training_data.Date > start_date]\n",
    "\n",
    "    test_data = df[df.Date > split_date]\n",
    "    Xtrain = training_data.iloc[:,1:-1]\n",
    "    #use 2018 season as test data\n",
    "    Xtest = test_data.iloc[:,1:-1]\n",
    "    ytrain = training_data.iloc[:,-1]\n",
    "    ytest = test_data.iloc[:,-1]\n",
    "    return Xtrain, Xtest, ytrain, ytest\n",
    "\n",
    "def run_logreg(Xtrain, Xtest, ytrain, ytest, scaler, cw):\n",
    "    \"\"\"Fit & tune a Logistic Regression model.\"\"\"\n",
    "    Xtrain_scld = scaler.fit_transform(Xtrain)\n",
    "    Xtest_scld = scaler.transform(Xtest)\n",
    "\n",
    "    param_grid = {'C': [0.001, 0.1, 1, 10, 100]}\n",
    "    logreg = LogisticRegression(solver = 'lbfgs', class_weight = cw)\n",
    "    #cross validation\n",
    "    logreg_cv = GridSearchCV(logreg, param_grid, cv = 10)\n",
    "    fitted = logreg_cv.fit(Xtrain_scld, ytrain)\n",
    "    print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "    print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "    return fitted, Xtrain_scld, Xtest_scld, ytrain, ytest\n",
    "    \n",
    "def run_report(fitted, Xtrain_scld, Xtest_scld, ytrain, ytest):\n",
    "    \"\"\"Generate Training and Test Classification Reports\"\"\"\n",
    "    ypred = fitted.predict(Xtest_scld)\n",
    "    ypred_train = fitted.predict(Xtrain_scld)\n",
    "    \n",
    "    yprobs = fitted.predict_proba(Xtest_scld)[:,1]\n",
    "    fpr, tpr, threshold = roc_curve(ytest,  yprobs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print(\"[Training Classification Report:]\")\n",
    "    print(classification_report(ytrain, ypred_train))\n",
    "    print('Training Accuracy: ',accuracy_score(ypred_train, ytrain))\n",
    "    print('')\n",
    "    print(\"[Test Classification Report:]\")\n",
    "    print(classification_report(ytest, ypred))\n",
    "    print('Test Accuracy: ',accuracy_score(ypred, ytest))\n",
    "    print('')\n",
    "    print('AUC: ', roc_auc)\n",
    "    return \n",
    "\n",
    "def split_fit_report(df, start_date = '2009-04-30',split_date = '2018-01',scaler = StandardScaler(), cw=None):\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test(df, start_date, split_date)\n",
    "    print('')\n",
    "    fitted, Xtrain_scld, Xtest_scld, ytrain, ytest = run_logreg(Xtrain, Xtest, ytrain, ytest, scaler, cw)\n",
    "    print('')\n",
    "    run_report(fitted, Xtrain_scld, Xtest_scld, ytrain, ytest)\n",
    "    return fitted, Xtrain_scld, Xtest_scld, ytrain, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall the Baseline Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[Training Classification Report:]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.55      0.27      0.36      9948\n",
    "           1       0.56      0.81      0.66     11596\n",
    "\n",
    "   micro avg       0.56      0.56      0.56     21544\n",
    "   macro avg       0.55      0.54      0.51     21544\n",
    "weighted avg       0.55      0.56      0.52     21544\n",
    "\n",
    "Training Accuracy:  0.5585313776457482\n",
    "\n",
    "[Test Classification Report:]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.55      0.24      0.34      1148\n",
    "           1       0.55      0.82      0.66      1283\n",
    "\n",
    "   micro avg       0.55      0.55      0.55      2431\n",
    "   macro avg       0.55      0.53      0.50      2431\n",
    "weighted avg       0.55      0.55      0.51      2431\n",
    "\n",
    "Test Accuracy:  0.5475113122171946"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC:  0.560206370630681"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1: Using class_weight = 'balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedReader.close>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = open('../PickledFiles/gamelogsS09', 'rb')\n",
    "gamelogsS09 = pickle.load(infile)\n",
    "infile.close\n",
    "\n",
    "infile = open('../PickledFiles/rel_cols', 'rb')\n",
    "rel_cols = pickle.load(infile)\n",
    "infile.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Logistic Regression Parameters: {'C': 0.001}\n",
      "Best score is 0.5404753063497958\n",
      "\n",
      "[Training Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.55      0.53      9948\n",
      "           1       0.59      0.55      0.57     11596\n",
      "\n",
      "   micro avg       0.55      0.55      0.55     21544\n",
      "   macro avg       0.55      0.55      0.55     21544\n",
      "weighted avg       0.55      0.55      0.55     21544\n",
      "\n",
      "Training Accuracy:  0.5487838841440772\n",
      "\n",
      "[Test Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.57      0.54      1148\n",
      "           1       0.57      0.52      0.54      1283\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      2431\n",
      "   macro avg       0.54      0.54      0.54      2431\n",
      "weighted avg       0.54      0.54      0.54      2431\n",
      "\n",
      "Test Accuracy:  0.5405183052241875\n",
      "\n",
      "AUC:  0.5602124810915183\n"
     ]
    }
   ],
   "source": [
    "fit= split_fit_report(gamelogsS09[rel_cols], cw='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC decreased.  Test accuracy deceased by about .007. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2: Using a Smaller Subset of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_stats = ['pctWminL_HSP','AvgFIPnoConst_HSP','AvgIP_HSP','AvgSB_H', 'AvgCS_H', 'AvgGDP_H',\n",
    "              'AvgDB_H', 'AvgOBP_H','AvgISO_H','AvgAper9_H', 'AvgPitchBABIP_H', 'AvgRelFIPnoConst_H',\n",
    "              'AvgEper9_H', 'AvgRunDiffAtHome_H']\n",
    "visit_stats = []\n",
    "for stat in home_stats[:-1]:\n",
    "    if stat.endswith('HSP'):\n",
    "        visit_stat = stat[:-3] +'VSP'\n",
    "    else:\n",
    "        visit_stat = stat[:-1] + 'V'\n",
    "    visit_stats.append(visit_stat)\n",
    "\n",
    "my_stats = ['Date'] +home_stats + visit_stats + ['AvgRunDiffOnRoad_V','AvgAttendance','HomeWin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Logistic Regression Parameters: {'C': 0.1}\n",
      "Best score is 0.540893056071296\n",
      "\n",
      "[Training Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.55      0.53      9948\n",
      "           1       0.59      0.55      0.57     11596\n",
      "\n",
      "   micro avg       0.55      0.55      0.55     21544\n",
      "   macro avg       0.55      0.55      0.55     21544\n",
      "weighted avg       0.55      0.55      0.55     21544\n",
      "\n",
      "Training Accuracy:  0.5498978834014111\n",
      "\n",
      "[Test Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.64      0.57      1148\n",
      "           1       0.59      0.48      0.53      1283\n",
      "\n",
      "   micro avg       0.55      0.55      0.55      2431\n",
      "   macro avg       0.56      0.56      0.55      2431\n",
      "weighted avg       0.56      0.55      0.55      2431\n",
      "\n",
      "Test Accuracy:  0.5528589058000822\n",
      "\n",
      "AUC:  0.5775756950309732\n"
     ]
    }
   ],
   "source": [
    "fit2 = split_fit_report(gamelogsS09[my_stats], cw = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the baseline model, the training accuracy decreases very slightly by about 0.001, but the test accuracy improves by about .006.  The area under the ROC curve also improved from 0.56 to 0.578."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 3: Using Data just from Current Season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedReader.close>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = open('../PickledFiles/gamelogs3', 'rb')\n",
    "gamelogs3 = pickle.load(infile)\n",
    "infile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the features that we added in AveragesSince2009 and RollingAverages\n",
    "\n",
    "#Avg errors per 9 innings - NEW COL \n",
    "gamelogs3['AvgEper9_H'] = gamelogs3['AvgE_H']*9/gamelogs3['AvgDefInnings_H']\n",
    "gamelogs3['AvgEper9_V'] = gamelogs3['AvgE_V']*9/gamelogs3['AvgDefInnings_V']\n",
    "\n",
    "#Avg strikeouts by offense per 9 innings - NEW COL\n",
    "gamelogs3['AvgKper9_H'] = gamelogs3['AvgK_H']*9/gamelogs3['AvgOffenseInnings_H']\n",
    "gamelogs3['AvgKper9_V'] = gamelogs3['AvgK_V']*9/gamelogs3['AvgOffenseInnings_V']\n",
    "\n",
    "#pct games won by starting pitcher - percent games lost\n",
    "gamelogs3['pctWminL_HSP'] = gamelogs3['pctW_HSP']-gamelogs3['pctL_HSP']\n",
    "gamelogs3['pctWminL_VSP'] = gamelogs3['pctW_VSP']-gamelogs3['pctL_VSP']\n",
    "\n",
    "#wild pitches t\n",
    "gamelogs3['HomeReliefWP'] = gamelogs3['VisitorWP'] - gamelogs3['WP_HSP']\n",
    "gamelogs3['VisitorReliefWP'] = gamelogs3['HomeWP'] - gamelogs3['WP_VSP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('../PickledFiles/gamelogs3_A', 'wb')\n",
    "pickle.dump(gamelogs3, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamelogs3_rel = gamelogs3[rel_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Logistic Regression Parameters: {'C': 0.001}\n",
      "Best score is 0.5462309691793539\n",
      "\n",
      "[Training Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.56      0.53      9948\n",
      "           1       0.59      0.55      0.57     11596\n",
      "\n",
      "   micro avg       0.55      0.55      0.55     21544\n",
      "   macro avg       0.55      0.55      0.55     21544\n",
      "weighted avg       0.56      0.55      0.55     21544\n",
      "\n",
      "Training Accuracy:  0.5534719643520237\n",
      "\n",
      "[Test Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.63      0.58      1148\n",
      "           1       0.61      0.51      0.55      1283\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      2431\n",
      "   macro avg       0.57      0.57      0.57      2431\n",
      "weighted avg       0.57      0.57      0.56      2431\n",
      "\n",
      "Test Accuracy:  0.5656108597285068\n",
      "\n",
      "AUC:  0.5934336987841541\n"
     ]
    }
   ],
   "source": [
    "fit3 = split_fit_report(gamelogs3_rel, cw = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test accuracy by about 1.8% (from 0.547 to 0.565). AUC increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the subset of variables I chose above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Logistic Regression Parameters: {'C': 0.001}\n",
      "Best score is 0.5480876346082436\n",
      "\n",
      "[Training Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.56      0.54      9948\n",
      "           1       0.59      0.55      0.57     11596\n",
      "\n",
      "   micro avg       0.55      0.55      0.55     21544\n",
      "   macro avg       0.55      0.55      0.55     21544\n",
      "weighted avg       0.56      0.55      0.55     21544\n",
      "\n",
      "Training Accuracy:  0.5533791310805792\n",
      "\n",
      "[Test Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.65      0.59      1148\n",
      "           1       0.61      0.50      0.55      1283\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      2431\n",
      "   macro avg       0.57      0.57      0.57      2431\n",
      "weighted avg       0.58      0.57      0.57      2431\n",
      "\n",
      "Test Accuracy:  0.5680789798436857\n",
      "\n",
      "AUC:  0.596847409571969\n"
     ]
    }
   ],
   "source": [
    "fit3sub = split_fit_report(gamelogs3_rel[my_stats], cw = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are not much different from using the full set of features on the same data set. Test accuracy and recall for class 0 slighlty higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 4: Using a Rolling Window (see RollingAverages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedReader.close>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = open('../PickledFiles/gamelogsRoll', 'rb')\n",
    "gamelogsRoll = pickle.load(infile)\n",
    "infile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glRoll_rel= gamelogsRoll[rel_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Logistic Regression Parameters: {'C': 0.1}\n",
      "Best score is 0.5506405495729669\n",
      "\n",
      "[Training Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.56      0.54      9948\n",
      "           1       0.60      0.56      0.57     11596\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     21544\n",
      "   macro avg       0.56      0.56      0.56     21544\n",
      "weighted avg       0.56      0.56      0.56     21544\n",
      "\n",
      "Training Accuracy:  0.5573245451169699\n",
      "\n",
      "[Test Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.62      0.58      1148\n",
      "           1       0.61      0.54      0.57      1283\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      2431\n",
      "   macro avg       0.58      0.58      0.58      2431\n",
      "weighted avg       0.58      0.58      0.58      2431\n",
      "\n",
      "Test Accuracy:  0.5771287535993418\n",
      "\n",
      "AUC:  0.6084457431814045\n"
     ]
    }
   ],
   "source": [
    "fit4,Xtrain_scld4,Xtest_scld4,ytrain,ytest = split_fit_report(glRoll_rel, cw = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best results yet. Training accuracy, test accuracy, and AUC are all slighlty higher when using the data computed using just current season data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Logistic Regression Parameters: {'C': 0.1}\n",
      "Best score is 0.553704047530635\n",
      "\n",
      "[Training Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.56      0.54      9948\n",
      "           1       0.59      0.56      0.57     11596\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     21544\n",
      "   macro avg       0.56      0.56      0.56     21544\n",
      "weighted avg       0.56      0.56      0.56     21544\n",
      "\n",
      "Training Accuracy:  0.5560712959524694\n",
      "\n",
      "[Test Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.62      0.58      1148\n",
      "           1       0.62      0.54      0.58      1283\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      2431\n",
      "   macro avg       0.58      0.58      0.58      2431\n",
      "weighted avg       0.58      0.58      0.58      2431\n",
      "\n",
      "Test Accuracy:  0.5795968737145207\n",
      "\n",
      "AUC:  0.6104343587139245\n"
     ]
    }
   ],
   "source": [
    "fit4sub = split_fit_report(gamelogsRoll[my_stats], cw = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the test accuracy and AUC slighlty higher than on full set of features with rolling average data. Precision, recall, and F1 score are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 5: Explicitly Compare the Stats of the Home and Away Team\n",
    "By subtracting visiting team's stats with the corresponding stat of the home team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pctSho_HSP',\n",
       " 'AvgIP_HSP',\n",
       " 'AvgWP_HSP',\n",
       " 'AvgFIPnoConst_HSP',\n",
       " 'AvgSH_H',\n",
       " 'AvgSF_H',\n",
       " 'AvgIBB_H',\n",
       " 'AvgSB_H',\n",
       " 'AvgCS_H',\n",
       " 'AvgGDP_H',\n",
       " 'AvgCI_H',\n",
       " 'AvgPassed_H',\n",
       " 'AvgDB_H',\n",
       " 'AvgTP_H',\n",
       " 'AvgReliefIP_H',\n",
       " 'AvgReliefWP_H',\n",
       " 'AvgOBP_H',\n",
       " 'AvgISO_H',\n",
       " 'AvgRelFIPnoConst_H',\n",
       " 'AvgPitchBABIP_H',\n",
       " 'AvgAper9_H',\n",
       " 'AvgEper9_H',\n",
       " 'pctWminL_HSP',\n",
       " 'AvgRunDiffAtHome_H']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_stats = [stat for stat in rel_cols if stat.endswith(('H','HSP'))]\n",
    "home_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for home_stat in home_stats[:-1]:\n",
    "    if home_stat.endswith('H'):\n",
    "        stat = home_stat[:-1]\n",
    "        visit_stat = stat + 'V'\n",
    "        diff_stat = stat + 'diff'\n",
    "    else:\n",
    "        stat = home_stat[:-3]\n",
    "        visit_stat = stat + 'VSP'\n",
    "        diff_stat = stat + 'SPdiff'\n",
    "    gamelogsRoll[diff_stat] = gamelogsRoll[home_stat] - gamelogsRoll[visit_stat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamelogsRoll['AvgSpread_diff'] = gamelogsRoll['AvgRunDiffAtHome_H'] - gamelogsRoll['AvgRunDiffOnRoad_V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_stats = [col for col in gamelogsRoll.columns if col.endswith('diff')]\n",
    "rel_stats_diff = ['Date'] + diff_stats  + ['AvgAttendance','HomeWin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Logistic Regression Parameters: {'C': 1}\n",
      "Best score is 0.5529613813590791\n",
      "\n",
      "[Training Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.56      0.54      9948\n",
      "           1       0.60      0.56      0.58     11596\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     21544\n",
      "   macro avg       0.56      0.56      0.56     21544\n",
      "weighted avg       0.56      0.56      0.56     21544\n",
      "\n",
      "Training Accuracy:  0.557927961381359\n",
      "\n",
      "[Test Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.59      0.57      1148\n",
      "           1       0.61      0.58      0.60      1283\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      2431\n",
      "   macro avg       0.58      0.58      0.58      2431\n",
      "weighted avg       0.59      0.58      0.58      2431\n",
      "\n",
      "Test Accuracy:  0.583710407239819\n",
      "\n",
      "AUC:  0.6090812311084919\n"
     ]
    }
   ],
   "source": [
    "fit5,_,_,_,_ = split_fit_report(gamelogsRoll[rel_stats_diff], cw = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_home_stats = [stat for stat in my_stats if stat.endswith(('H','HSP'))]\n",
    "my_diff_stats = []\n",
    "for home_stat in my_home_stats[:-1]:\n",
    "    if home_stat.endswith('H'):\n",
    "        stat = home_stat[:-1]\n",
    "        visit_stat = stat + 'V'\n",
    "        diff_stat = stat + 'diff'\n",
    "    else:\n",
    "        stat = home_stat[:-3]\n",
    "        visit_stat = stat + 'VSP'\n",
    "        diff_stat = stat + 'SPdiff'\n",
    "    my_diff_stats.append(diff_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_diff_stats = ['Date'] + my_diff_stats  + ['AvgSpread_diff','AvgAttendance', 'HomeWin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Logistic Regression Parameters: {'C': 0.1}\n",
      "Best score is 0.555235796509469\n",
      "\n",
      "[Training Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.56      0.54      9948\n",
      "           1       0.59      0.56      0.57     11596\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     21544\n",
      "   macro avg       0.56      0.56      0.56     21544\n",
      "weighted avg       0.56      0.56      0.56     21544\n",
      "\n",
      "Training Accuracy:  0.5569067953954697\n",
      "\n",
      "[Test Classification Report:]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.59      0.57      1148\n",
      "           1       0.61      0.58      0.60      1283\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      2431\n",
      "   macro avg       0.58      0.58      0.58      2431\n",
      "weighted avg       0.59      0.58      0.58      2431\n",
      "\n",
      "Test Accuracy:  0.5845331139448786\n",
      "\n",
      "AUC:  0.6115193049826055\n"
     ]
    }
   ],
   "source": [
    "fit5sub, _,_,_,_ = split_fit_report(gamelogsRoll[my_diff_stats],cw = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC, test precision, recall, f1-score, all at their highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_rels = rel_cols\n",
    "all_rels = rel_cols[:-1]+rel_stats_diff[1:-2]+['HomeWin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('../PickledFiles/gamelogsRoll_ext', 'wb')\n",
    "pickle.dump(gamelogsRoll, outfile)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open('../PickledFiles/rel_diffs', 'wb')\n",
    "pickle.dump(rel_stats_diff, outfile)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open('../PickledFiles/rel_cols_extended', 'wb')\n",
    "pickle.dump(rel_cols, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
